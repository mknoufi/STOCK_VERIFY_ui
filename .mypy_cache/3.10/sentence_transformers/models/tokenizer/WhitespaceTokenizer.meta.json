{"data_mtime":1767697673,"dep_hashes":["3d3248b10e36c9817c142339fa4314a3705451a7","5d3d8056897e7fb087f00133c6b798fe8c948162","b11f9d14b2f96f2081893cd2bcfdf515c53a310c","2949e71b8686931200359909a6eb4b0135b73400","73f33247bc6fffcb02f45414fe63c64eb526bb1d","c4bb948f827357f82df210178e2f16276ffbcb9d","b1f9b5b27355da4bb8919b27a894a748e410e171","ddf753d7b8e8c270291880218f90ed2b62a277ee","92ff0dee4665f29f429cb3727aeb9bcb45c22a61","212d34d9e06c4cc566a576d3c3cffa805e92d065","6ddb6f13b3590b1708d05529fb99637945ca75dc"],"dep_lines":[9,7,1,3,4,5,6,1,1,1,1],"dep_prios":[5,5,5,10,10,10,10,5,30,30,30],"dependencies":["sentence_transformers.models.tokenizer.WordTokenizer","collections.abc","__future__","collections","json","os","string","builtins","_typeshed","abc","typing"],"error_lines":[],"hash":"329ee9352575f059213bda3bc22b7d0119290920","id":"sentence_transformers.models.tokenizer.WhitespaceTokenizer","ignore_all":true,"interface_hash":"8aa2c881fa90a44ddaa1be7ec6b08aa987a7b45f","mtime":1766134135,"options":{"other_options":"631b02669c43f539d4be2012709fe971181cf59a","platform":"darwin"},"path":"/Users/noufi1/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/tokenizer/WhitespaceTokenizer.py","plugin_data":null,"size":2483,"suppressed":[],"version_id":"1.19.1"}