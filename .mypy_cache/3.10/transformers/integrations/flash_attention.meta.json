{"data_mtime":1767697671,"dep_hashes":["b92ef02a3502e0836c1e49bc5d424600f75b9d2b","9145786e662a8fe40aa5e0bde30b50200a1bb591","f1d0c295f4a87de9cf2889f4af2a6fc762164dc7","6ddb6f13b3590b1708d05529fb99637945ca75dc","d59d48f5a0e384aec2eeb1945c489cd3a7957090","ddf753d7b8e8c270291880218f90ed2b62a277ee","212d34d9e06c4cc566a576d3c3cffa805e92d065","c178cc53cb428a2c6ebc6ba780676a177a6f90e9","a0cbcaec349b176731b014bdff5162aa1fa225b2","4a2a0d8b8b5c9679e07f99bf8b7fbe5c679cbd27"],"dep_lines":[6,5,6,1,3,1,1,1,1,1],"dep_prios":[10,5,20,5,10,5,30,30,30,30],"dependencies":["transformers.utils.logging","transformers.modeling_flash_attention_utils","transformers.utils","typing","torch","builtins","abc","logging","torch._tensor","torch.nn.modules.module"],"error_lines":[],"hash":"498454158f1d4cff062a72a7046134cc517e0126","id":"transformers.integrations.flash_attention","ignore_all":true,"interface_hash":"192567fc36c26398f2ab324c33436a25680c3552","mtime":1766134131,"options":{"other_options":"631b02669c43f539d4be2012709fe971181cf59a","platform":"darwin"},"path":"/Users/noufi1/Library/Python/3.9/lib/python/site-packages/transformers/integrations/flash_attention.py","plugin_data":null,"size":3125,"suppressed":[],"version_id":"1.19.1"}